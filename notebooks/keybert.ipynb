{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7446857-abbc-4dde-b493-10bb21773f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_candidates(text, module):\n",
    "    if module == \"okt\":\n",
    "        okt = Okt()\n",
    "        tokenized_doc = okt.pos(text)\n",
    "        tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Noun'])\n",
    "    elif module == \"mecab\":\n",
    "        mecab = Mecab()\n",
    "        tokenized_doc = mecab.pos(text)\n",
    "        tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] in ['NNP','NNG','SL']])\n",
    "    else:\n",
    "        raise ValueError(\"no module\")\n",
    "    \n",
    "    n_gram_range = (1,1)\n",
    "    count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
    "    candidates = count.get_feature_names_out()\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def dist_keywords(doc_embedding, candidate_embeddings, candidates, top_n):\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "    return keywords\n",
    "\n",
    "\n",
    "def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\n",
    "    # 문서와 각 키워드들 간의 유사도\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
    "                                            candidate_embeddings)\n",
    "\n",
    "    # 코사인 유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 pick.\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [words[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # 각 키워드들 중에서 가장 덜 유사한 키워드들간의 조합을 계산\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "    if candidate:\n",
    "        keywords = [words_vals[idx] for idx in candidate]\n",
    "        return keywords\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # 문서와 각 키워드들 간의 유사도가 적혀있는 리스트\n",
    "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    word_similarity = cosine_similarity(candidate_embeddings)\n",
    "\n",
    "    # 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # keywords_idx = [2]\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "\n",
    "    # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.\n",
    "    # ex) top_n = 5라면, 아래의 loop는 4번 반복됨.\n",
    "    for _ in range(top_n - 1):\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # MMR을 계산\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # keywords & candidates를 업데이트\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "    \n",
    "    if keywords_idx:\n",
    "        keywords = [words[idx] for idx in keywords_idx]\n",
    "        return keywords\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_keyword(text, top_n, module, model):\n",
    "    candidates = get_candidates(text, module)\n",
    "    doc_embedding = model.encode([text])\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "    \n",
    "    results = list()\n",
    "    results.append(dist_keywords(doc_embedding, candidate_embeddings, candidates, top_n=top_n))\n",
    "    results.append(max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=top_n, nr_candidates=top_n*2))\n",
    "    results.append(mmr(doc_embedding, candidate_embeddings, candidates, top_n=top_n, diversity=0.8))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37ec1aa9-02e8-4ce2-ae0d-5a46fc812962",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecdfe394-d7c7-451e-a3d3-c2247fad71f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== mecab 결과 =====\n",
      "Cosine Similarity : ['협의회', '계획', '회의', '간담회', '월요일']\n",
      "Max Sum Similarity : ['화요일', '정책', '계획', '회의', '월요일']\n",
      "Maximal Marginal Relevance : ['월요일', '사업', '계획', '대변인', '문화']\n",
      "total : 계획 사업 정책 협의회 월요일 문화 회의 간담회 화요일 대변인\n"
     ]
    }
   ],
   "source": [
    "text = \"안녕하십니까? 문체부 대변인입니다. 1월 28일 월요일 정례브리핑을 시작하겠습니다. 금일 브리핑 순서는 주요 장차관 일정, 두 번째 보도자료 배포계획, 세 번째 현안, 질의·응답순으로 진행토록 그렇게 하겠습니다. 금주 장차관 주요일정입니다. 먼저, 장관입니다. 내일 제4회 국무회의에 참가하시고요. 수요일에는 제20차 장애인정책조정위원회에 참석하십니다. 그리고 31일 목요일에는 콘텐츠산업 청년종사자 간담회를 천안에서 개최할 계획입니다. 1차관 관련 부분입니다. 수요일에 제2차 생활SOC협의회에 참석하시고, 2차관은 1월 28일 금일 설 명절 계기 민생현장 방문이 예정되어 있습니다. 금주 보도자료 배포계획입니다. 오늘 브리핑 계획은 1월 문화가 있는 날과 불법게임장 근절 관계기관 간담회 개최, 2건입니다. 이 부분은 잠시 후에 상세하게 말씀드리고. 이번 주 주요 보도자료는 화요일에는 ‘2018 국민여가활동조사’ 결과 발표가 나갈 예정이고, 수요일에는 ‘2019 문화누리카드 발급’ 관련 보도자료, 그다음에 국립박물관·미술관 설 명절 계기 문화행사 개최 계획이 안내될 예정입니다. 31일은 지역콘텐츠 진흥 종사자 간담회 개최 계획이 있고, ‘2018 광고산업 통계조사’ 결과가 발표될 예정입니다. 금요일에는 ‘2019 지자체 개최 국제경기대회 공모사업’ 결과가 발표될 예정입니다. 오늘 브리핑은 먼저, 2019년 첫 번째 문화가 있는 날 행사 계획 관련입니다.\"\n",
    "\n",
    "\n",
    "results = get_keyword(text, 5, \"mecab\", model)\n",
    "unique_keywords = []\n",
    "for result in results:\n",
    "    unique_keywords.extend(result)\n",
    "\n",
    "str_keyword = ' '.join(map(str, list(set(unique_keywords))))\n",
    "print(\"===== mecab 결과 =====\")\n",
    "print(\"Cosine Similarity :\", results[0])\n",
    "print(\"Max Sum Similarity :\", results[1])\n",
    "print(\"Maximal Marginal Relevance :\", results[2])\n",
    "print(\"total :\",str_keyword)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
